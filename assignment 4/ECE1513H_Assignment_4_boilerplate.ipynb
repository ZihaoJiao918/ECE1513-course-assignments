{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pxMUmlEENLXc"
   },
   "source": [
    "Let's first get the imports out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0yDWKUrCBiX"
   },
   "outputs": [],
   "source": [
    "import array\n",
    "import gzip\n",
    "import itertools\n",
    "import numpy\n",
    "import numpy.random as npr\n",
    "import os\n",
    "import struct\n",
    "import time\n",
    "from os import path\n",
    "import urllib.request\n",
    "\n",
    "import jax.numpy as np\n",
    "from jax.api import jit, grad\n",
    "from jax.config import config\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIp--T57NGrU"
   },
   "source": [
    "The following cell contains boilerplate code to download and load MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Du24u5vtDEIn"
   },
   "outputs": [],
   "source": [
    "_DATA = \"/tmp/\"\n",
    "\n",
    "def _download(url, filename):\n",
    "  \"\"\"Download a url to a file in the JAX data temp directory.\"\"\"\n",
    "  if not path.exists(_DATA):\n",
    "    os.makedirs(_DATA)\n",
    "  out_file = path.join(_DATA, filename)\n",
    "  if not path.isfile(out_file):\n",
    "    urllib.request.urlretrieve(url, out_file)\n",
    "    print(\"downloaded {} to {}\".format(url, _DATA))\n",
    "\n",
    "\n",
    "def _partial_flatten(x):\n",
    "  \"\"\"Flatten all but the first dimension of an ndarray.\"\"\"\n",
    "  return numpy.reshape(x, (x.shape[0], -1))\n",
    "\n",
    "\n",
    "def _one_hot(x, k, dtype=numpy.float32):\n",
    "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "  return numpy.array(x[:, None] == numpy.arange(k), dtype)\n",
    "\n",
    "\n",
    "def mnist_raw():\n",
    "  \"\"\"Download and parse the raw MNIST dataset.\"\"\"\n",
    "  # CVDF mirror of http://yann.lecun.com/exdb/mnist/\n",
    "  base_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
    "\n",
    "  def parse_labels(filename):\n",
    "    with gzip.open(filename, \"rb\") as fh:\n",
    "      _ = struct.unpack(\">II\", fh.read(8))\n",
    "      return numpy.array(array.array(\"B\", fh.read()), dtype=numpy.uint8)\n",
    "\n",
    "  def parse_images(filename):\n",
    "    with gzip.open(filename, \"rb\") as fh:\n",
    "      _, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
    "      return numpy.array(array.array(\"B\", fh.read()),\n",
    "                      dtype=numpy.uint8).reshape(num_data, rows, cols)\n",
    "\n",
    "  for filename in [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\",\n",
    "                   \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"]:\n",
    "    _download(base_url + filename, filename)\n",
    "\n",
    "  train_images = parse_images(path.join(_DATA, \"train-images-idx3-ubyte.gz\"))\n",
    "  train_labels = parse_labels(path.join(_DATA, \"train-labels-idx1-ubyte.gz\"))\n",
    "  test_images = parse_images(path.join(_DATA, \"t10k-images-idx3-ubyte.gz\"))\n",
    "  test_labels = parse_labels(path.join(_DATA, \"t10k-labels-idx1-ubyte.gz\"))\n",
    "\n",
    "  return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "\n",
    "def mnist(create_outliers=False):\n",
    "  \"\"\"Download, parse and process MNIST data to unit scale and one-hot labels.\"\"\"\n",
    "  train_images, train_labels, test_images, test_labels = mnist_raw()\n",
    "\n",
    "  train_images = _partial_flatten(train_images) / numpy.float32(255.)\n",
    "  test_images = _partial_flatten(test_images) / numpy.float32(255.)\n",
    "  train_labels = _one_hot(train_labels, 10)\n",
    "  test_labels = _one_hot(test_labels, 10)\n",
    "\n",
    "  if create_outliers:\n",
    "    mum_outliers = 30000\n",
    "    perm = numpy.random.RandomState(0).permutation(mum_outliers)\n",
    "    train_images[:mum_outliers] = train_images[:mum_outliers][perm]\n",
    "\n",
    "  return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "def shape_as_image(images, labels, dummy_dim=False):\n",
    "  target_shape = (-1, 1, 28, 28, 1) if dummy_dim else (-1, 28, 28, 1)\n",
    "  return np.reshape(images, target_shape), labels\n",
    "\n",
    "train_images, train_labels, test_images, test_labels = mnist(create_outliers=False)\n",
    "num_train = train_images.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LdTKppnqIHeV"
   },
   "source": [
    "# **Problem 1**\n",
    "\n",
    "This function computes the output of a fully-connected neural network (i.e., multilayer perceptron) by iterating over all of its layers and:\n",
    "\n",
    "1. taking the `activations` of the previous layer (or the input itself for the first hidden layer) to compute the `outputs` of a linear classifier. Recall the lectures: `outputs` is what we wrote $z=w\\cdot x + b$ where $x$ is the input to the linear classifier. \n",
    "2. applying a non-linear activation. Here we will use $tanh$.\n",
    "\n",
    "Complete the following cell to compute `outputs` and `activations`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGU4xK-iHjhx"
   },
   "outputs": [],
   "source": [
    "def predict(params, inputs):\n",
    "  activations = inputs\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = np.dot(activations,w)+b\n",
    "    activations = np.tanh(outputs)\n",
    "\n",
    "  final_w, final_b = params[-1]\n",
    "  logits = np.dot(activations, final_w) + final_b\n",
    "  return logits - logsumexp(logits, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UgE_NBb9JNDI"
   },
   "source": [
    "The following cell computes the loss of our model. Here we are using cross-entropy combined with a softmax but the implementation uses the `LogSumExp` trick for numerical stability. This is why our previous function `predict` returns the logits to which we substract the `logsumexp` of logits. We discussed this in class but you can read more about it [here](https://blog.feedly.com/tricks-of-the-trade-logsumexp/).\n",
    "\n",
    "Complete the return line. Recall that the loss is defined as :\n",
    "$$ l(X, Y) = -\\frac{1}{n} \\sum_{i\\in 1..n}  \\sum_{j\\in 1.. K}y_j^{(i)} \\log(f_j(x^{(i)})) = -\\frac{1}{n} \\sum_{i\\in 1..n}  \\sum_{j\\in 1.. K}y_j^{(i)} \\log\\left(\\frac{z_j^{(i)}}{\\sum_{k\\in 1..K}z_k^{(i)}}\\right) $$\n",
    "where $X$ is a matrix containing a batch of $n$ training inputs, and $Y$ a matrix containing a batch of one-hot encoded labels defined over $K$ labels. Here $z_j^{(i)}$ is the logits (i.e., input to the softmax) of the model on the example $i$ of our batch of training examples $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlgdP72hH9ly"
   },
   "outputs": [],
   "source": [
    "def loss(params, batch):\n",
    "  inputs, targets = batch\n",
    "  preds = predict(params, inputs)\n",
    "  \n",
    "  return -np.sum(targets*preds)/len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILZ-q5PTMohU"
   },
   "source": [
    "The following cell defines the accuracy of our model and how to initialize its parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UDjCuIGzIAjf"
   },
   "outputs": [],
   "source": [
    "def accuracy(params, batch):\n",
    "  inputs, targets = batch\n",
    "  target_class = np.argmax(targets, axis=1)\n",
    "  predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
    "  return np.mean(predicted_class == target_class)\n",
    "\n",
    "def init_random_params(layer_sizes, rng=npr.RandomState(0)):\n",
    "  scale = 0.1\n",
    "  return [(scale * rng.randn(m, n), scale * rng.randn(n))\n",
    "          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "as46_qd5NWMA"
   },
   "source": [
    "The following line defines our architecture with the number of neurons contained in each fully-connected layer (the first layer has 784 neurons because MNIST images are 28*28=784 pixels and the last layer has 10 neurons because MNIST has 10 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZzv-4dHNV4D"
   },
   "outputs": [],
   "source": [
    "layer_sizes = [784, 1024, 128, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKroKJ6TOETY"
   },
   "source": [
    "The following cell creates a Python generator for our dataset. It outputs one batch of $n$ training examples at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_6lLT1klOMIn"
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "def data_stream():\n",
    "  rng = npr.RandomState(0)\n",
    "  while True:\n",
    "    perm = rng.permutation(num_train)\n",
    "    for i in range(num_batches):\n",
    "      batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "      yield train_images[batch_idx], train_labels[batch_idx]\n",
    "batches = data_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lm-UbcYZOOci"
   },
   "source": [
    "We are now ready to define our optimizer. Here we use mini-batch stochastic gradient descent. Complete `<w UPDATE RULE>` and `<b UPDATE RULE>` using the update rule we saw in class. Recall that `dw` is the partial derivative of the `loss` with respect to `w` and `learning_rate` is the learning rate of gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8EktCm-OvFh"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "@jit\n",
    "def update(params, batch):\n",
    "  grads = grad(loss)(params, batch)\n",
    "  return [(w -learning_rate*dw,b - learning_rate*db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tqo4M7uNOvzo"
   },
   "source": [
    "This is now the proper training loop for our fully-connected neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "fgrHTrafDHAE",
    "outputId": "bd142984-9ef0-435f-bdff-fd069541b594"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 2.66 sec\n",
      "Training set accuracy 0.9605833292007446\n",
      "Test set accuracy 0.9535000324249268\n",
      "Epoch 1 in 1.08 sec\n",
      "Training set accuracy 0.9768000245094299\n",
      "Test set accuracy 0.9665000438690186\n",
      "Epoch 2 in 1.09 sec\n",
      "Training set accuracy 0.9838666915893555\n",
      "Test set accuracy 0.9708000421524048\n",
      "Epoch 3 in 1.10 sec\n",
      "Training set accuracy 0.9891666769981384\n",
      "Test set accuracy 0.9743000268936157\n",
      "Epoch 4 in 1.09 sec\n",
      "Training set accuracy 0.9914166927337646\n",
      "Test set accuracy 0.9727000594139099\n",
      "Epoch 5 in 1.09 sec\n",
      "Training set accuracy 0.9946333169937134\n",
      "Test set accuracy 0.9759000539779663\n",
      "Epoch 6 in 1.09 sec\n",
      "Training set accuracy 0.9973666667938232\n",
      "Test set accuracy 0.9770000576972961\n",
      "Epoch 7 in 1.09 sec\n",
      "Training set accuracy 0.9978833198547363\n",
      "Test set accuracy 0.9777000546455383\n",
      "Epoch 8 in 1.08 sec\n",
      "Training set accuracy 0.9990333318710327\n",
      "Test set accuracy 0.9782000184059143\n",
      "Epoch 9 in 1.07 sec\n",
      "Training set accuracy 0.9993166923522949\n",
      "Test set accuracy 0.9795000553131104\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "params = init_random_params(layer_sizes)\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for _ in range(num_batches):\n",
    "    params = update(params, next(batches))\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  train_acc = accuracy(params, (train_images, train_labels))\n",
    "  test_acc = accuracy(params, (test_images, test_labels))\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9yFvjncWk8G"
   },
   "source": [
    "# **Problem 2**\n",
    "\n",
    "Before we get started, we need to import two small libraries that contain boilerplate code for common neural network layer types and for optimizers like mini-batch SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvLxNfXtXCRb"
   },
   "outputs": [],
   "source": [
    "from jax.experimental import optimizers\n",
    "from jax.experimental import stax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNwMlXqfXI8G"
   },
   "source": [
    "Here is a fully-connected neural network architecture, like the one of Problem 1, but this time defined with `stax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4wu1XqFds4X"
   },
   "outputs": [],
   "source": [
    "init_random_params, predict = stax.serial(\n",
    "    stax.Conv(256, (5,5),strides = (2,2)),\n",
    "    stax.Relu,\n",
    "    stax.Conv(128, (3,3)),\n",
    "    stax.Relu,\n",
    "    stax.Conv(32, (3,3)),\n",
    "    stax.Relu,\n",
    "    stax.MaxPool((2,2)),\n",
    "    stax.Flatten,\n",
    "    stax.Dense(1024),\n",
    "    stax.Relu,\n",
    "    stax.Dense(128),\n",
    "    stax.Relu,\n",
    "    stax.Dense(10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nEW_OcOCdwFX"
   },
   "source": [
    "We redefine the cross-entropy loss for this model. As done in Problem 1, complete the return line below (it's identical). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQEeOtAEdvYn"
   },
   "outputs": [],
   "source": [
    "def loss(params, batch):\n",
    "  inputs, targets = batch\n",
    "  logits = predict(params, inputs)\n",
    "  preds  = stax.logsoftmax(logits)\n",
    "  return -np.sum(targets*preds)/len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ZBxTvCweJbN"
   },
   "source": [
    "Next, we define the mini-batch SGD optimizer, this time with the optimizers library in JAX. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "peG-cAZ0eGTG"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.08\n",
    "opt_init, opt_update, get_params = optimizers.sgd(learning_rate)\n",
    "\n",
    "@jit\n",
    "def update(_, i, opt_state, batch):\n",
    "  params = get_params(opt_state)\n",
    "  return opt_update(i, grad(loss)(params, batch), opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVx2h8lqeoTD"
   },
   "source": [
    "The next cell contains our training loop, very similar to Problem 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCs9w6hW-ylB"
   },
   "source": [
    "learning rate, batch size, number\n",
    "of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "41Y6wwFzb-mk",
    "outputId": "c00441f7-29e9-49c1-9007-67ff6933315b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set loss, accuracy (%): (0.05, 98.35)\n",
      "Test set loss, accuracy (%): (0.04, 98.84)\n",
      "Test set loss, accuracy (%): (0.05, 98.43)\n",
      "Test set loss, accuracy (%): (0.03, 98.91)\n",
      "Test set loss, accuracy (%): (0.03, 99.16)\n",
      "Test set loss, accuracy (%): (0.03, 99.26)\n",
      "Test set loss, accuracy (%): (0.04, 98.87)\n",
      "Test set loss, accuracy (%): (0.03, 99.08)\n",
      "Test set loss, accuracy (%): (0.03, 99.01)\n",
      "Test set loss, accuracy (%): (0.03, 99.11)\n",
      "Test set loss, accuracy (%): (0.03, 99.24)\n",
      "Test set loss, accuracy (%): (0.03, 99.14)\n",
      "Test set loss, accuracy (%): (0.03, 99.25)\n",
      "Test set loss, accuracy (%): (0.03, 99.26)\n",
      "Test set loss, accuracy (%): (0.03, 99.30)\n",
      "Test set loss, accuracy (%): (0.03, 99.33)\n",
      "Test set loss, accuracy (%): (0.04, 99.34)\n",
      "Test set loss, accuracy (%): (0.04, 99.33)\n",
      "Test set loss, accuracy (%): (0.04, 99.30)\n",
      "Test set loss, accuracy (%): (0.04, 99.31)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "key = random.PRNGKey(123)\n",
    "_, init_params = init_random_params(key, (-1, 28, 28, 1))\n",
    "opt_state = opt_init(init_params)\n",
    "itercount = itertools.count()\n",
    "\n",
    "acc_list=[]\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "  for _ in range(num_batches):\n",
    "    opt_state = update(key, next(itercount), opt_state, shape_as_image(*next(batches)))\n",
    "\n",
    "  params = get_params(opt_state)\n",
    "  test_acc = accuracy(params, shape_as_image(test_images, test_labels))\n",
    "  test_loss = loss(params, shape_as_image(test_images, test_labels))\n",
    "  print('Test set loss, accuracy (%): ({:.2f}, {:.2f})'.format(test_loss, 100 * test_acc))\n",
    "  acc_list.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DQsu8aNU8C0s",
    "outputId": "7d0d905c-126e-4c96-9269-355d1672b150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 99.091%\n",
      "Variance: 0.27937233\n"
     ]
    }
   ],
   "source": [
    "result = np.array(acc_list)\n",
    "average_score = result.mean()*100\n",
    "score_std = result.std()*100\n",
    "\n",
    "print(\"Average Accuracy: \" + str(average_score)+\"%\") \n",
    "print(\"Variance: \" + str(score_std))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ECE1513H_Assignment_4_boilerplate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
