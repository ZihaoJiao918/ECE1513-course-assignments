{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_partB.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bztoT4mw_Xfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "from jax import grad\n",
        "import jax.numpy as jaxnp\n",
        "from jax import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5vrvMvv_lPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_data(number_of_sample_entries):\n",
        "\n",
        " #initialize size\n",
        "  data = np.ndarray((number_of_sample_entries,3))\n",
        "  label = np.zeros((number_of_sample_entries,3))\n",
        "  \n",
        "  #class 1\n",
        "  class_1 = number_of_sample_entries//3\n",
        "  data[0:class_1, :] = np.random.uniform(low=1, high=5,size=(class_1,3))\n",
        "  label[0:class_1, :] = np.array([1,0,0])\n",
        "  #class 2\n",
        "  class_2 = class_1 + number_of_sample_entries//3\n",
        "  data[class_1:class_2, :] = np.random.uniform(low=10, high=15,size=(class_1,3))\n",
        "  label[class_1:class_2, :] = np.array([0,1,0])\n",
        "  #class 3\n",
        "  class_3 = class_2 + number_of_sample_entries//3\n",
        "  data[class_2:class_3, :] = np.random.uniform(low=20, high=25,size=(class_1,3))\n",
        "  label[class_2:class_3, :] = np.array([0,0,1])\n",
        "\n",
        "  #build it into datafram\n",
        "  x = pd.DataFrame(data)\n",
        "  labels = pd.DataFrame(label)\n",
        "  #split data and shuffle data\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, labels, test_size = 0.3, shuffle=True)\n",
        "\n",
        "  #return it to array(jax)\n",
        "\n",
        "  label_test_jax = jaxnp.array(y_test)\n",
        "  label_train_jax = jaxnp.array(y_train)\n",
        "  data_train_jax = jaxnp.array(X_train)\n",
        "  data_test_jax = jaxnp.array(X_test)\n",
        "\n",
        "\n",
        "\n",
        "  return label_test_jax,label_train_jax,data_train_jax,data_test_jax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqp2X7WJ_-K0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_test,label_train,data_train,data_test = generate_data(300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mVHj36OAASZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(X):\n",
        "   return jaxnp.maximum(X,0)\n",
        "\n",
        "\n",
        "\n",
        "def relu_derivative(x):\n",
        "  return jaxnp.where(x>0, 1, 0)\n",
        "\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "  list_res=[]\n",
        "  for i in range(len(x)):\n",
        "    res = jaxnp.exp(x[i])/jaxnp.sum(jaxnp.exp(x[i]))\n",
        "    list_res.append(res)\n",
        "  return jaxnp.array(list_res)\n",
        "\n",
        "# def softmax(x):\n",
        "#   return (jaxnp.exp(x)) / jaxnp.sum(jaxnp.exp(x))\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1 + jaxnp.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUmEH5T2AHFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize our neural network parameters.\n",
        "key = random.PRNGKey(0)\n",
        "key, W_key, b_key = random.split(key, 3)\n",
        "\n",
        "params = {}\n",
        "params['w_1'] = random.normal(W_key,(3,3))\n",
        "params['b_1'] = random.normal(b_key,(3,))\n",
        "params['w_2'] = random.normal(W_key,(3,3))\n",
        "params['b_2'] = random.normal(b_key,(3,))\n",
        "params['w_3'] = random.normal(W_key,(3,3))\n",
        "params['b_3'] = random.normal(b_key,(3,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyoP3mYDAI0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function (data, t, w_1, b_1, w_2, b_2, w_3, b_3):\n",
        "  J_in = jaxnp.dot(data, jaxnp.transpose(w_1))  + b_1\n",
        "  J_out = relu(J_in)\n",
        "  K_in = jaxnp.dot(J_out,jaxnp.transpose(w_2)) + b_2\n",
        "  K_out = sigmoid(K_in)\n",
        "  L_in = jaxnp.dot(K_out,jaxnp.transpose(w_3)) + b_3\n",
        "  L_out = softmax(L_in)\n",
        "\n",
        "  loss = (1./data.shape[0]) * jaxnp.sum(-t * jaxnp.log(L_out) - (1 - t) * jaxnp.log(1 - L_out))\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoSqNyVmD3uS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dw_1 = grad(loss_function,2)\n",
        "db_1 = grad(loss_function,3)\n",
        "\n",
        "dw_2 = grad(loss_function,4)\n",
        "db_2 = grad(loss_function,5)\n",
        "\n",
        "dw_3 = grad(loss_function,6)\n",
        "db_3 = grad(loss_function,7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szil_rvlCQag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = 0.002\n",
        "iterations = 5000\n",
        "cost_list = []\n",
        "for i in range(iterations):\n",
        "    loss = loss_function(data_train, label_train, params['w_1'],params['b_1'],params['w_2'],params['b_2'],params['w_3'],params['b_3'])\n",
        "    \n",
        "    #weights update\n",
        "    params['w_1'] -= alpha * dw_1(data_train, label_train, params['w_1'],params['b_1'],params['w_2'],params['b_2'],params['w_3'],params['b_3'])\n",
        "    params['w_2'] -= alpha * dw_2(data_train, label_train, params['w_1'],params['b_1'],params['w_2'],params['b_2'],params['w_3'],params['b_3'])\n",
        "    params['w_3'] -= alpha * dw_3(data_train, label_train, params['w_1'],params['b_1'],params['w_2'],params['b_2'],params['w_3'],params['b_3'])\n",
        "    #bias update\n",
        "    params['b_1'] -= alpha * db_1(data_train, label_train, params['w_1'],params['b_1'],params['w_2'],params['b_2'],params['w_3'],params['b_3'])\n",
        "    params['b_2'] -= alpha * db_2(data_train, label_train, params['w_1'],params['b_1'],params['w_2'],params['b_2'],params['w_3'],params['b_3'])\n",
        "    params['b_3'] -= alpha * db_3(data_train, label_train, params['w_1'],params['b_1'],params['w_2'],params['b_2'],params['w_3'],params['b_3'])\n",
        "\n",
        "\n",
        "    cost_list.append(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2-_qCjoVsLl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "534cc0fb-ee17-450a-9579-20907573f10d"
      },
      "source": [
        "plt.plot(cost_list)\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOQW9iIjPKehFRHxOQS8i4nMKehERn/sfer5BcjQP2ncAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYZVYF6sY7WI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward (data, params):\n",
        "  J_in = jaxnp.dot(data, jaxnp.transpose(params['w_1']))  + params['b_1']\n",
        "  J_out = relu(J_in)\n",
        "  K_in = jaxnp.dot(J_out,jaxnp.transpose(params['w_2'])) + params['b_2']\n",
        "  K_out = sigmoid(K_in)\n",
        "  L_in = jaxnp.dot(K_out,jaxnp.transpose(params['w_3'])) + params['b_3']\n",
        "  L_out = softmax(L_in)\n",
        "  return L_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d_nruEpZLPj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "a8bd1df1-4658-4b3f-87ce-bc47d5f2636e"
      },
      "source": [
        "prediction = forward(data_test,params)\n",
        "pred_df = pd.DataFrame(prediction,columns=['[d   a','   t   ','  a]'])\n",
        "label_df = pd.DataFrame(label_test,columns=['[L  a','b  e','l  s]'])\n",
        "result = pd.concat([pred_df,label_df],axis=1)\n",
        "result.head(10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>[d   a</th>\n",
              "      <th>t</th>\n",
              "      <th>a]</th>\n",
              "      <th>[L  a</th>\n",
              "      <th>b  e</th>\n",
              "      <th>l  s]</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.433730</td>\n",
              "      <td>0.207069</td>\n",
              "      <td>0.359200</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.433730</td>\n",
              "      <td>0.207069</td>\n",
              "      <td>0.359200</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.433730</td>\n",
              "      <td>0.207069</td>\n",
              "      <td>0.359200</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.422104</td>\n",
              "      <td>0.275083</td>\n",
              "      <td>0.302813</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.426585</td>\n",
              "      <td>0.244961</td>\n",
              "      <td>0.328454</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.433730</td>\n",
              "      <td>0.207069</td>\n",
              "      <td>0.359200</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.428353</td>\n",
              "      <td>0.235910</td>\n",
              "      <td>0.335737</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.433730</td>\n",
              "      <td>0.207069</td>\n",
              "      <td>0.359200</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.433730</td>\n",
              "      <td>0.207069</td>\n",
              "      <td>0.359200</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.428978</td>\n",
              "      <td>0.232726</td>\n",
              "      <td>0.338297</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     [d   a      t           a]  [L  a  b  e  l  s]\n",
              "0  0.433730  0.207069  0.359200    0.0   0.0    1.0\n",
              "1  0.433730  0.207069  0.359200    0.0   0.0    1.0\n",
              "2  0.433730  0.207069  0.359200    0.0   1.0    0.0\n",
              "3  0.422104  0.275083  0.302813    0.0   1.0    0.0\n",
              "4  0.426585  0.244961  0.328454    0.0   0.0    1.0\n",
              "5  0.433730  0.207069  0.359200    0.0   0.0    1.0\n",
              "6  0.428353  0.235910  0.335737    0.0   1.0    0.0\n",
              "7  0.433730  0.207069  0.359200    0.0   1.0    0.0\n",
              "8  0.433730  0.207069  0.359200    0.0   0.0    1.0\n",
              "9  0.428978  0.232726  0.338297    0.0   1.0    0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}